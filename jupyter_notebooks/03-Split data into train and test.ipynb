{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Notebook 3: Split data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Generate train and test sets for feature engineering.\n",
        "* Do Correlation and PPS analysis to visualize the relationship between different variables in the dataset.\n",
        "\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/cleaned/house_prices_records_cleaned.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Generate Train and Test sets from cleaned data, saved under outputs/datasets/cleaned/train and outputs/datasets/cleaned/test\n",
        "\n",
        "## Comment\n",
        "\n",
        "* For readers of this notebook, previously the name of the main dataframe used in this project has been called 'records_df'. From this notebook on, the main dataframe will simply be 'df', with variations when we use a subset of the df for a particular purpose. In this notebook, we split df into train and test sets, and their respective names will be 'TrainSet' and 'TestSet'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c",
        "tags": []
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df_raw_path = \"outputs/datasets/cleaned/house_prices_records_cleaned.csv\"\n",
        "df = pd.read_csv(df_raw_path)\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Split train and test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "In the cell below, the target variable 'SalePrice' is separated out from the rest of the data, and the split produces both a train and test set for the features (TrainSet and TestSet) and the target (train_target and test_target).\n",
        "\n",
        "The reason for changing the code from the walkthrough is that in the train_test_split() function, I was passing df['SalePrice'] as the target variable, but also including it in my df, which means my target variable 'SalePrice' would be present in both my features and targets, which is not what I would typically want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "features = df.drop('SalePrice', axis=1)  # drop the target variable from the feature set\n",
        "target = df['SalePrice']\n",
        "\n",
        "TrainSet, TestSet, train_target, test_target = train_test_split(\n",
        "    features,\n",
        "    target,\n",
        "    test_size=0.2,\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "print(f\"TrainSet shape: {TrainSet.shape} \\nTestSet shape: {TestSet.shape}\")\n"
      ]
    },
    {
      "source": [
        "As we see in the output, the train set has 1168 rows which is 80% of the data, and the test set accounts for the remaining 20%."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "We then save the train and test set respectively in their folders."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TrainSet.to_csv(\"outputs/datasets/cleaned/train/TrainSetCleaned.csv\", index=False)\n",
        "TestSet.to_csv(\"outputs/datasets/cleaned/test/TestSetCleaned.csv\", index=False)"
      ]
    },
    {
      "source": [
        "And we also save the datasets where we put the target variable."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_target.to_csv(\"outputs/datasets/cleaned/train/TrainSetTarget.csv\", index=False)\n",
        "test_target.to_csv(\"outputs/datasets/cleaned/test/TestSetTarget.csv\", index=False)"
      ]
    },
    {
      "source": [],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "# Load cleaned training and test sets"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "Train set"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "TrainSet = pd.read_csv(\"outputs/datasets/cleaned/train/TrainSetCleaned.csv\")\n",
        "TrainSet.head()"
      ]
    },
    {
      "source": [
        "Test set"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "TestSet = pd.read_csv(\"outputs/datasets/cleaned/test/TestSetCleaned.csv\")\n",
        "TestSet.head()"
      ],
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "# Data Exploration"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "In this section we are interested to evaluate which potential transformation we could do in our variables."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "%matplotlib inline"
      ],
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pandas_profiling import ProfileReport\n",
        "pandas_report = ProfileReport(df=TrainSet, minimal=True)\n",
        "pandas_report.to_notebook_iframe()"
      ]
    },
    {
      "source": [],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "# Correlation and PPS Analysis"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "Note: Since the first notebooks in this project are not identical to those of the walkthrough project Churnometer, namely we did data cleaning before the EDA (which is a valid approach), we will therefore assess correlation levels and PPS here before starting the feature engineering process in the next notebook."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ppscore as pps\n",
        "\n",
        "\n",
        "def heatmap_corr(df, threshold, figsize=(20, 12), font_annot=8):\n",
        "    if len(df.columns) > 1:\n",
        "        mask = np.zeros_like(df, dtype=np.bool)\n",
        "        mask[np.triu_indices_from(mask)] = True\n",
        "        mask[abs(df) < threshold] = True\n",
        "\n",
        "        fig, axes = plt.subplots(figsize=figsize)\n",
        "        sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
        "                    mask=mask, cmap='viridis', annot_kws={\"size\": font_annot}, ax=axes,\n",
        "                    linewidth=0.5\n",
        "                    )\n",
        "        axes.set_yticklabels(df.columns, rotation=0)\n",
        "        plt.ylim(len(df.columns), 0)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def heatmap_pps(df, threshold, figsize=(20, 12), font_annot=8):\n",
        "    if len(df.columns) > 1:\n",
        "        mask = np.zeros_like(df, dtype=np.bool)\n",
        "        mask[abs(df) < threshold] = True\n",
        "        fig, ax = plt.subplots(figsize=figsize)\n",
        "        ax = sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
        "                         mask=mask, cmap='rocket_r', annot_kws={\"size\": font_annot},\n",
        "                         linewidth=0.05, linecolor='grey')\n",
        "        plt.ylim(len(df.columns), 0)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def CalculateCorrAndPPS(df):\n",
        "    df_corr_spearman = df.corr(method=\"spearman\")\n",
        "    df_corr_pearson = df.corr(method=\"pearson\")\n",
        "\n",
        "    pps_matrix_raw = pps.matrix(df)\n",
        "    pps_matrix = pps_matrix_raw.filter(['x', 'y', 'ppscore']).pivot(columns='x', index='y', values='ppscore')\n",
        "\n",
        "    pps_score_stats = pps_matrix_raw.query(\"ppscore < 1\").filter(['ppscore']).describe().T\n",
        "    print(\"PPS threshold - check PPS score IQR to decide threshold for heatmap \\n\")\n",
        "    print(pps_score_stats.round(3))\n",
        "\n",
        "    return df_corr_pearson, df_corr_spearman, pps_matrix\n",
        "\n",
        "\n",
        "def DisplayCorrAndPPS(df_corr_pearson, df_corr_spearman, pps_matrix, CorrThreshold, PPS_Threshold,\n",
        "                      figsize=(20, 12), font_annot=8):\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"* Analyse how the target variable for your ML models are correlated with other variables (features and target)\")\n",
        "    print(\"* Analyse multi-colinearity, that is, how the features are correlated among themselves\")\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"*** Heatmap: Spearman Correlation ***\")\n",
        "    print(\"It evaluates monotonic relationship \\n\")\n",
        "    heatmap_corr(df=df_corr_spearman, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"*** Heatmap: Pearson Correlation ***\")\n",
        "    print(\"It evaluates the linear relationship between two continuous variables \\n\")\n",
        "    heatmap_corr(df=df_corr_pearson, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"*** Heatmap: Power Predictive Score (PPS) ***\")\n",
        "    print(f\"PPS detects linear or non-linear relationships between two columns.\\n\"\n",
        "          f\"The score ranges from 0 (no predictive power) to 1 (perfect predictive power) \\n\")\n",
        "    heatmap_pps(df=pps_matrix, threshold=PPS_Threshold, figsize=figsize, font_annot=font_annot)"
      ]
    },
    {
      "source": [
        "Calculate Correlations and Power Predictive Score"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "df_corr_pearson, df_corr_spearman, pps_matrix = CalculateCorrAndPPS(df)"
      ]
    },
    {
      "source": [
        "What does this mean?\n",
        "\n",
        "- count: There are 462 pairs of variables for which the PPS was calculated.\n",
        "- mean: On average, the PPS among all the pairs is around 0.061. This indicates that, on average, variables have a relatively weak predictive power.\n",
        "- std: The standard deviation is 0.105, which indicates a relatively large variation in the PPS scores across different pairs of variables.\n",
        "- min: The lowest PPS score among all pairs is 0. This means that for at least one pair, there is no predictive power from one variable to another.\n",
        "- 25% (1st Quartile): 25% of the pairs have a PPS of 0.\n",
        "- 50% (Median): Half of the pairs have a PPS of 0. This indicates that for most variable pairs, there's no predictive power or it's very weak.\n",
        "- 75% (3rd Quartile): 75% of the pairs have a PPS up to 0.083. This further demonstrates that the majority of variable pairs have very low predictive power.\n",
        "- max: The highest PPS score among all pairs is 0.625. This means that there is at least one pair of variables where one variable can predict the other with a fair amount of accuracy (62.5%).\n",
        "\n",
        "The output suggests that most pairs of variables have very low or no predictive power, but there are few pairs with relatively high predictive power (max PPS is 0.625)."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "Therefore we display at heatmaps to help visualize the relationship among different variables in the dataset. Note the relatively high thresholds set as the maps become difficult to read when all moderately correlated variables are shown."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "DisplayCorrAndPPS(df_corr_pearson = df_corr_pearson,\n",
        "                  df_corr_spearman = df_corr_spearman, \n",
        "                  pps_matrix = pps_matrix,\n",
        "                  CorrThreshold = 0.6, PPS_Threshold =0.3,\n",
        "                  figsize=(12,10), font_annot=10)"
      ]
    },
    {
      "source": [
        "A couple of observations from the heatmaps:\n",
        "- 'TotalBsmtSF' and '1stFlrSF':\n",
        "These variables are highly correlated with each other as shown by their high Spearman (0.83) and Pearson scores (0.82). This means that they tend to increase or decrease together. For example, a larger basement ('TotalBsmtSF') often comes with a larger first floor ('1stFlrSF'). Their Power Predictive Score (PPS) of 0.44 indicates a moderate predictive power. In other words, knowledge of 'TotalBsmtSF' would be moderately useful in predicting the value of '1stFlrSF'.\n",
        "\n",
        "- 'YearBuilt' and 'GarageYrBlt':\n",
        "Again, these variables are highly correlated, with Spearman (0.85) and Pearsob (0.78). This implies that houses tend to have their garages built the same year as the house itself. The high PPS of 0.59 indicates that the year a house was built ('YearBuilt') is a good predictor of the year its garage was built ('GarageYrBlt'), and vice versa.\n",
        "\n",
        "- This list is not exhaustive, there are several other pairs of variables that show similar tendencies.\n",
        "\n",
        "The findings suggest that each pair of variables carries similar information, indicating multicollinearity."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "---"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The analysis shows overall low predictive power in most pairs of variables, with a few variables with higher PPS.\n",
        "- There's a tendency of multicollinearity several variable pairs, which is important to keep in mind in the upcoming modeling stages."
      ]
    },
    {
      "source": [
        "# Next step"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "- In the next notebook we engineer our features. \n",
        "- Since we have 4 categorical variables, these will need to be transformed into numerical variables. \n",
        "- We want all numerical variables to be normally distributed, and will therefore use numerical transformation.\n",
        "- Lastly we will handle features that are highly correlated with one another, which we saw in this notebook."
      ],
      "cell_type": "markdown",
      "metadata": {}
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python381264bit3812pyenvc44d0ba3d4d74926a1a78ae71e39f6ac"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12-final"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}