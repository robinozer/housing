{
  "cells": [
    {
      "source": [
        "# **Notebook 4: Modeling & Evaluation**"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Fit and evaluate a regression model to predict prices on houses.\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/collection/house_prices_records.csv\n",
        "* Instructions on which features to use for data cleaning and feature engineering can be found in respective notebooks.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Train set (features and target)\n",
        "* Test set (features and target)\n",
        "* Data cleaning and Feature Engineering pipeline\n",
        "* Modeling pipeline\n",
        "* Feature importance plot\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "'/workspaces/housing/jupyter_notebooks'"
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "You set a new current directory\n"
        }
      ],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "'/workspaces/housing'"
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Step 1: Load data"
      ]
    },
    {
      "source": [
        "Here we load the dataset and separate the target variable ('y') from the predictor variables ('X').\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "(1460, 23)\n"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "   1stFlrSF  2ndFlrSF  BedroomAbvGr BsmtExposure  BsmtFinSF1 BsmtFinType1  \\\n0       856     854.0           3.0           No         706          GLQ   \n1      1262       0.0           3.0           Gd         978          ALQ   \n2       920     866.0           3.0           Mn         486          GLQ   \n\n   BsmtUnfSF  EnclosedPorch  GarageArea GarageFinish  ...  LotArea  \\\n0        150            0.0         548          RFn  ...     8450   \n1        284            NaN         460          RFn  ...     9600   \n2        434            0.0         608          RFn  ...    11250   \n\n   LotFrontage MasVnrArea  OpenPorchSF  OverallCond  OverallQual  TotalBsmtSF  \\\n0         65.0      196.0           61            5            7          856   \n1         80.0        0.0            0            8            6         1262   \n2         68.0      162.0           42            5            7          920   \n\n   WoodDeckSF  YearBuilt  YearRemodAdd  \n0         0.0       2003          2003  \n1         NaN       1976          1976  \n2         NaN       2001          2002  \n\n[3 rows x 23 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1stFlrSF</th>\n      <th>2ndFlrSF</th>\n      <th>BedroomAbvGr</th>\n      <th>BsmtExposure</th>\n      <th>BsmtFinSF1</th>\n      <th>BsmtFinType1</th>\n      <th>BsmtUnfSF</th>\n      <th>EnclosedPorch</th>\n      <th>GarageArea</th>\n      <th>GarageFinish</th>\n      <th>...</th>\n      <th>LotArea</th>\n      <th>LotFrontage</th>\n      <th>MasVnrArea</th>\n      <th>OpenPorchSF</th>\n      <th>OverallCond</th>\n      <th>OverallQual</th>\n      <th>TotalBsmtSF</th>\n      <th>WoodDeckSF</th>\n      <th>YearBuilt</th>\n      <th>YearRemodAdd</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>856</td>\n      <td>854.0</td>\n      <td>3.0</td>\n      <td>No</td>\n      <td>706</td>\n      <td>GLQ</td>\n      <td>150</td>\n      <td>0.0</td>\n      <td>548</td>\n      <td>RFn</td>\n      <td>...</td>\n      <td>8450</td>\n      <td>65.0</td>\n      <td>196.0</td>\n      <td>61</td>\n      <td>5</td>\n      <td>7</td>\n      <td>856</td>\n      <td>0.0</td>\n      <td>2003</td>\n      <td>2003</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1262</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>Gd</td>\n      <td>978</td>\n      <td>ALQ</td>\n      <td>284</td>\n      <td>NaN</td>\n      <td>460</td>\n      <td>RFn</td>\n      <td>...</td>\n      <td>9600</td>\n      <td>80.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>6</td>\n      <td>1262</td>\n      <td>NaN</td>\n      <td>1976</td>\n      <td>1976</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>920</td>\n      <td>866.0</td>\n      <td>3.0</td>\n      <td>Mn</td>\n      <td>486</td>\n      <td>GLQ</td>\n      <td>434</td>\n      <td>0.0</td>\n      <td>608</td>\n      <td>RFn</td>\n      <td>...</td>\n      <td>11250</td>\n      <td>68.0</td>\n      <td>162.0</td>\n      <td>42</td>\n      <td>5</td>\n      <td>7</td>\n      <td>920</td>\n      <td>NaN</td>\n      <td>2001</td>\n      <td>2002</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows Ã— 23 columns</p>\n</div>"
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "df = (pd.read_csv(\"outputs/datasets/collection/house_prices_records.csv\"))\n",
        "\n",
        "# Separate predictors and target\n",
        "X = df.drop(['SalePrice'], axis=1)\n",
        "y = df['SalePrice']\n",
        "\n",
        "print(X.shape)\n",
        "X.head(3)"
      ]
    },
    {
      "source": [
        "---"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "# Step 2: ML Pipeline with all data"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "## ML pipeline for Data Cleaning and Feature Engineering"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Feature Engineering\n",
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "\n",
        "\n",
        "def PipelineDataCleaningAndFeatureEngineering():\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(encoding_method='arbitrary',\n",
        "                                                     variables=['BsmtExposure', 'BsmtFinType1', 'GarageFinish', 'KitchenQual'])),\n",
        "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None,\n",
        "         method=\"spearman\", threshold=0.6, selection_method=\"variance\")),\n",
        "    ])\n",
        "\n",
        "    return pipeline_base\n",
        "\n",
        "\n",
        "PipelineDataCleaningAndFeatureEngineering()\n"
      ]
    },
    {
      "source": [
        "## ML Pipeline for Modelling and Hyperparameter Optimisation"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feat Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Feat Selection\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# ML algorithms for regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "\n",
        "# adapt the function to regression\n",
        "def PipelineReg(model):\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"feat_selection\", SelectFromModel(model)),\n",
        "        (\"model\", model),\n",
        "    ])\n",
        "\n",
        "    return pipeline_base\n"
      ]
    },
    {
      "source": [
        "Custom Class for Hyperparameter Optimisation"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "class HyperparameterOptimizationSearch:\n",
        "\n",
        "    def __init__(self, models, params):\n",
        "        self.models = models\n",
        "        self.params = params\n",
        "        self.keys = models.keys()\n",
        "        self.grid_searches = {}\n",
        "\n",
        "    def fit(self, X, y, cv, n_jobs, verbose=1, scoring=None, refit=False):\n",
        "        for key in self.keys:\n",
        "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
        "\n",
        "            model = PipelineReg(self.models[key])  # changed this line to my own function from cell above\n",
        "            params = self.params[key]\n",
        "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
        "                              verbose=verbose, scoring=scoring, )\n",
        "            gs.fit(X, y)\n",
        "            self.grid_searches[key] = gs\n",
        "\n",
        "    def score_summary(self, sort_by='mean_score'):\n",
        "        def row(key, scores, params):\n",
        "            d = {\n",
        "                'estimator': key,\n",
        "                'min_score': min(scores),\n",
        "                'max_score': max(scores),\n",
        "                'mean_score': np.mean(scores),\n",
        "                'std_score': np.std(scores),\n",
        "            }\n",
        "            return pd.Series({**params, **d})\n",
        "\n",
        "        rows = []\n",
        "        for k in self.grid_searches:\n",
        "            params = self.grid_searches[k].cv_results_['params']\n",
        "            scores = []\n",
        "            for i in range(self.grid_searches[k].cv):\n",
        "                key = \"split{}_test_score\".format(i)\n",
        "                r = self.grid_searches[k].cv_results_[key]\n",
        "                scores.append(r.reshape(len(params), 1))\n",
        "\n",
        "            all_scores = np.hstack(scores)\n",
        "            for p, s in zip(params, all_scores):\n",
        "                rows.append((row(k, s, p)))\n",
        "\n",
        "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
        "        columns = ['estimator', 'min_score',\n",
        "                   'mean_score', 'max_score', 'std_score']\n",
        "        columns = columns + [c for c in df.columns if c not in columns]\n",
        "        return df[columns], self.grid_searches"
      ]
    },
    {
      "source": [
        "## Split Train and Test Set"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    random_state=0,\n",
        ")\n",
        "\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n"
      ]
    },
    {
      "source": [
        "Note: we do not handle target imbalance at this point since the target in this project is a continuous variable. It's possible to come back to this step later and add a cell to normalize the distribution of the target variable to fit the model better."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "## Grid Search CV - Sklearn"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "Use standard hyperparameters to find most suitable algorithm"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "models_quick_search = {\n",
        "    \"LinearRegression\": LinearRegression(),\n",
        "    \"Ridge\": Ridge(),\n",
        "    \"Lasso\": Lasso(),\n",
        "    \"DecisionTreeRegressor\": DecisionTreeRegressor(random_state=0),\n",
        "    \"RandomForestRegressor\": RandomForestRegressor(random_state=0),\n",
        "    \"GradientBoostingRegressor\": GradientBoostingRegressor(random_state=0),\n",
        "    \"XGBRegressor\": XGBRegressor(random_state=0),\n",
        "    \"SVR\": SVR(),\n",
        "}\n",
        "\n",
        "params_quick_search = {\n",
        "    \"LinearRegression\": {},\n",
        "    \"Ridge\": {},\n",
        "    \"Lasso\": {},\n",
        "    \"DecisionTreeRegressor\": {},\n",
        "    \"RandomForestRegressor\": {},\n",
        "    \"GradientBoostingRegressor\": {},\n",
        "    \"XGBRegressor\": {},\n",
        "    \"SVR\": {},\n",
        "}"
      ]
    },
    {
      "source": [
        "And then Quick GridSearch CV"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import make_scorer, mean_squared_error\n",
        "\n",
        "# We want to minimize MSE, hence greater_is_better=False.\n",
        "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
        "\n",
        "search = HyperparameterOptimizationSearch(models=models_quick_search, params=params_quick_search)\n",
        "search.fit(X_train, y_train, scoring=scorer, n_jobs=-1, cv=5)"
      ]
    },
    {
      "source": [
        "Check results"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
        "grid_search_summary "
      ]
    },
    {
      "source": [
        "### Do an extensive search on the most suitable algorithm to find the best hyperparameter configuration."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "Define model and parameters, for Extensive Search"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models_search = {\n",
        "    \"XGBRegressor\": XGBRegressor(random_state=0),\n",
        "}\n",
        "\n",
        "# documentation to help on hyperparameter list: \n",
        "# https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn\n",
        "\n",
        "params_search = {\n",
        "    \"XGBRegressor\": {\n",
        "        'model__learning_rate': [1e-1, 1e-2, 1e-3], \n",
        "        'model__max_depth': [3, 10, None],\n",
        "        'model__n_estimators': [50, 100, 200],  # add more parameters for tuning\n",
        "    },\n",
        "}"
      ]
    },
    {
      "source": [
        "Extensive GridSearch CV"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, make_scorer\n",
        "\n",
        "# We want to minimize MSE, hence greater_is_better=False.\n",
        "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
        "\n",
        "search = HyperparameterOptimizationSearch(models=models_search, params=params_search)\n",
        "search.fit(X_train, y_train, scoring=scorer, n_jobs=-1, cv=5)"
      ]
    },
    {
      "source": [
        "Check results"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "Here we get a summary table showing the results from the GridSearchCV operation. It includes the mean_score, which we can use to compare the performance of different hyperparameters configurations. The one with the highest (or lowest, if your metric is an error/loss to be minimized) mean_score would be the optimal choice.\n",
        "\n",
        "Keep in mind that the score here is the mean_squared_error with greater_is_better=False, so the model configuration with the most negative mean score is the best one, as it signifies the lowest mean squared error."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
        "grid_search_summary "
      ]
    },
    {
      "source": [
        "Get best model name programmatically"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = grid_search_summary.iloc[0,0]\n",
        "best_model"
      ]
    },
    {
      "source": [
        "Parameters for best model. This contain a dictionary of the optimal hyperparameters for the best model as determined by the GridSearchCV. It doesn't contain the model itself, but only the parameters that gave the best performance during the cross-validation grid search"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_parameters = grid_search_pipelines[best_model].best_params_\n",
        "best_parameters"
      ]
    },
    {
      "source": [
        "Get the complete pipeline of the best model, including all preprocessing steps and the estimator itself, with the parameters set to their optimal values:"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_rgr = grid_search_pipelines[best_model].best_estimator_\n",
        "pipeline_rgr"
      ]
    },
    {
      "source": [
        "In other words, pipeline_rgr is ready for immediate use on data, while best_parameters would need to be used to set the parameters of a new instance of the model before fitting it to the data."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "## Assess feature importance"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create DataFrame to display feature importance\n",
        "df_feature_importance = (pd.DataFrame(data={\n",
        "    'Feature': X_train.columns[pipeline_rgr['feat_selection'].get_support()],\n",
        "    'Importance': pipeline_rgr['model'].feature_importances_})\n",
        "    .sort_values(by='Importance', ascending=False)\n",
        ")\n",
        "\n",
        "# re-assign best_features order\n",
        "best_features = df_feature_importance['Feature'].to_list()\n",
        "\n",
        "# Most important features statement and plot\n",
        "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
        "      f\"The model was trained on them: \\n{df_feature_importance['Feature'].to_list()}\")\n",
        "\n",
        "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
        "plt.show()"
      ]
    },
    {
      "source": [
        "Note to self: This should create a bar plot of feature importances, showing the most important features on the top.\n",
        "\n",
        "Please note that you'll need to be sure that your pipeline_rgr model's estimator ('model' step in your pipeline) indeed has a .feature_importances_ attribute. The XGBoost, RandomForest, and GradientBoosting regressors do, but others like SVMs and linear models do not. In the latter case, this cell will fail.\n",
        "\n",
        "Please be careful when interpreting these feature importances - they reflect the model's internal working, but they might not directly correspond to \"importance\" in a business or logical sense. They also don't indicate the direction of influence a feature has (whether high feature value increases or decreases the predicted target), only the magnitude of that influence."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "## Evaluate Pipeline on Train and Test Sets"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "def regression_metrics(X, y, pipeline):\n",
        "    prediction = pipeline.predict(X)\n",
        "\n",
        "    print('---  Regression Metrics  ---')\n",
        "    print(f\"Mean Absolute Error (MAE): {mean_absolute_error(y, prediction)}\")\n",
        "    print(f\"Mean Squared Error (MSE): {mean_squared_error(y, prediction)}\")\n",
        "    print(f\"Root Mean Squared Error (RMSE): {mean_squared_error(y, prediction, squared=False)}\")\n",
        "    print(f\"R-squared (R^2): {r2_score(y, prediction)}\")\n",
        "\n",
        "def reg_performance(X_train, y_train, X_test, y_test, pipeline):\n",
        "    print(\"#### Train Set #### \\n\")\n",
        "    regression_metrics(X_train, y_train, pipeline)\n",
        "\n",
        "    print(\"\\n#### Test Set ####\\n\")\n",
        "    regression_metrics(X_test, y_test, pipeline)\n"
      ]
    },
    {
      "source": [
        "This version of the function above calculates and prints out regression metrics, which are more relevant for evaluating how well a regression model performs. These include MAE, MSE, RMSE, and R^2.\n",
        "\n",
        "After running this function, we should see these metrics printed out for both our training set and test set. This will give us an idea of how well our model is doing on both the data it was trained on and on new, unseen data."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reg_performance(X_train=X_train, y_train=y_train,\n",
        "                X_test=X_test, y_test=y_test,\n",
        "                pipeline=pipeline_rgr)\n",
        "\n",
        "# The function reg_performance will print out the regression metrics \n",
        "# for our training and test datasets using the pipeline we trained."
      ]
    },
    {
      "source": [
        "# Step 3: Refit pipeline with best features"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "source": [
        "# Next step: Notebook 2"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "In notebook 2 we will move on the the next next step in our data analysis process: Exploratory Data Analysis (EDA)."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [],
      "cell_type": "markdown",
      "metadata": {}
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python381264bit3812pyenvc44d0ba3d4d74926a1a78ae71e39f6ac"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12-final"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}