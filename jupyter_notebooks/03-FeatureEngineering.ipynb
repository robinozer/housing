{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Notebook 3: Feature Engineering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Generate train and test sets for feature engineering.\n",
        "* Engineer features for Classification, Regression and Cluster models\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/cleaned/house_prices_records_cleaned.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Generate Train and Test sets from cleaned data, saved under outputs/datasets/cleaned/train and outputs/datasets/cleaned/test\n",
        "\n",
        "## Conclusions\n",
        "\n",
        "* In case you have any additional comments that don't fit in the previous bullets, please state them here. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "'/workspaces/housing/jupyter_notebooks'"
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "You set a new current directory\n"
        }
      ],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "'/workspaces/housing'"
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "   1stFlrSF  2ndFlrSF  BedroomAbvGr BsmtExposure  BsmtFinSF1 BsmtFinType1  \\\n0       856       854             3           No         706          GLQ   \n1      1262         0             3           Gd         978          ALQ   \n2       920       866             3           Mn         486          GLQ   \n\n   BsmtUnfSF  GarageArea GarageFinish  GarageYrBlt  ...  LotArea LotFrontage  \\\n0        150         548          RFn         2003  ...     8450          65   \n1        284         460          RFn         1976  ...     9600          80   \n2        434         608          RFn         2001  ...    11250          68   \n\n   MasVnrArea  OpenPorchSF  OverallCond  OverallQual  TotalBsmtSF  YearBuilt  \\\n0         196           61            5            7          856       2003   \n1           0            0            8            6         1262       1976   \n2         162           42            5            7          920       2001   \n\n   YearRemodAdd  SalePrice  \n0          2003     208500  \n1          1976     181500  \n2          2002     223500  \n\n[3 rows x 22 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1stFlrSF</th>\n      <th>2ndFlrSF</th>\n      <th>BedroomAbvGr</th>\n      <th>BsmtExposure</th>\n      <th>BsmtFinSF1</th>\n      <th>BsmtFinType1</th>\n      <th>BsmtUnfSF</th>\n      <th>GarageArea</th>\n      <th>GarageFinish</th>\n      <th>GarageYrBlt</th>\n      <th>...</th>\n      <th>LotArea</th>\n      <th>LotFrontage</th>\n      <th>MasVnrArea</th>\n      <th>OpenPorchSF</th>\n      <th>OverallCond</th>\n      <th>OverallQual</th>\n      <th>TotalBsmtSF</th>\n      <th>YearBuilt</th>\n      <th>YearRemodAdd</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>856</td>\n      <td>854</td>\n      <td>3</td>\n      <td>No</td>\n      <td>706</td>\n      <td>GLQ</td>\n      <td>150</td>\n      <td>548</td>\n      <td>RFn</td>\n      <td>2003</td>\n      <td>...</td>\n      <td>8450</td>\n      <td>65</td>\n      <td>196</td>\n      <td>61</td>\n      <td>5</td>\n      <td>7</td>\n      <td>856</td>\n      <td>2003</td>\n      <td>2003</td>\n      <td>208500</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1262</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Gd</td>\n      <td>978</td>\n      <td>ALQ</td>\n      <td>284</td>\n      <td>460</td>\n      <td>RFn</td>\n      <td>1976</td>\n      <td>...</td>\n      <td>9600</td>\n      <td>80</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>6</td>\n      <td>1262</td>\n      <td>1976</td>\n      <td>1976</td>\n      <td>181500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>920</td>\n      <td>866</td>\n      <td>3</td>\n      <td>Mn</td>\n      <td>486</td>\n      <td>GLQ</td>\n      <td>434</td>\n      <td>608</td>\n      <td>RFn</td>\n      <td>2001</td>\n      <td>...</td>\n      <td>11250</td>\n      <td>68</td>\n      <td>162</td>\n      <td>42</td>\n      <td>5</td>\n      <td>7</td>\n      <td>920</td>\n      <td>2001</td>\n      <td>2002</td>\n      <td>223500</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 22 columns</p>\n</div>"
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df_raw_path = \"outputs/datasets/cleaned/house_prices_records_cleaned.csv\"\n",
        "df = pd.read_csv(df_raw_path)\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Split train and test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the cell below, the target variable 'SalePrice' is separated out from the rest of the data, and the split produces both a train and test set for the features (TrainSet and TestSet) and the target (train_target and test_target).\n",
        "\n",
        "The reason for changing the code from the walkthrough is that in the train_test_split() function, I was passing df(['SalePrice']) as the target variable, but also including it in my df, which means my target variable 'SalePrice' would be present in both my features and targets, which is not what I would typically want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "TrainSet shape: (1168, 21) \nTestSet shape: (292, 21)\n"
        }
      ],
      "source": [
        "features = df.drop('SalePrice', axis=1)  # drop the target variable from the feature set\n",
        "target = df['SalePrice']\n",
        "\n",
        "TrainSet, TestSet, train_target, test_target = train_test_split(\n",
        "    features,\n",
        "    target,\n",
        "    test_size=0.2,\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "print(f\"TrainSet shape: {TrainSet.shape} \\nTestSet shape: {TestSet.shape}\")\n"
      ]
    },
    {
      "source": [
        "As we see in the output, the train set has 1168 rows which is 80% of the data, and the test set accounts for the remaining 20%."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "We then save the train and test set respectively in their folders."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "TrainSet.to_csv(\"outputs/datasets/cleaned/train/TrainSetCleaned.csv\", index=False)\n",
        "TestSet.to_csv(\"outputs/datasets/cleaned/test/TestSetCleaned.csv\", index=False)"
      ],
      "cell_type": "code",
      "metadata": {},
      "execution_count": 23,
      "outputs": []
    },
    {
      "source": [
        "And we also save the datasets where we put the target variable."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_target.to_csv(\"outputs/datasets/cleaned/train/TrainSetTarget.csv\", index=False)\n",
        "test_target.to_csv(\"outputs/datasets/cleaned/test/TestSetTarget.csv\", index=False)"
      ]
    },
    {
      "source": [],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "# Correlation and PPS Analysis"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "* Since we did data cleaning before the EDA and analyses, we don’t expect any changes in correlation levels and PPS compared to the previous notebook."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "# FEATURE ENGINEERING"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "Now we move on to Feature Engineering.\n",
        "\n",
        "In this section we will handle outliers, encode categorical variables, transform features and create new features. \n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "# Load cleaned training and test sets"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "   1stFlrSF  2ndFlrSF  BedroomAbvGr BsmtExposure  BsmtFinSF1 BsmtFinType1  \\\n0      2515         0             4           No        1219          Rec   \n1       958       620             3           No         403          BLQ   \n2       979       224             3           No         185          LwQ   \n3      1156       866             4           No         392          BLQ   \n4       525         0             3           No           0          Unf   \n\n   BsmtUnfSF  GarageArea GarageFinish  GarageYrBlt  ...  KitchenQual LotArea  \\\n0        816         484          Unf         1975  ...           TA   32668   \n1        238         240          Unf         1941  ...           Fa    9490   \n2        524         352          Unf         1950  ...           Gd    7015   \n3        768         505          Fin         1977  ...           TA   10005   \n4        525         264          Unf         1971  ...           TA    1680   \n\n   LotFrontage  MasVnrArea  OpenPorchSF  OverallCond  OverallQual  \\\n0           69           0            0            3            6   \n1           79           0            0            7            6   \n2           69         161            0            4            5   \n3           83         299          117            5            7   \n4           21         381            0            5            6   \n\n   TotalBsmtSF  YearBuilt  YearRemodAdd  \n0         2035       1957          1975  \n1          806       1941          1950  \n2          709       1950          1950  \n3         1160       1977          1977  \n4          525       1971          1971  \n\n[5 rows x 21 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1stFlrSF</th>\n      <th>2ndFlrSF</th>\n      <th>BedroomAbvGr</th>\n      <th>BsmtExposure</th>\n      <th>BsmtFinSF1</th>\n      <th>BsmtFinType1</th>\n      <th>BsmtUnfSF</th>\n      <th>GarageArea</th>\n      <th>GarageFinish</th>\n      <th>GarageYrBlt</th>\n      <th>...</th>\n      <th>KitchenQual</th>\n      <th>LotArea</th>\n      <th>LotFrontage</th>\n      <th>MasVnrArea</th>\n      <th>OpenPorchSF</th>\n      <th>OverallCond</th>\n      <th>OverallQual</th>\n      <th>TotalBsmtSF</th>\n      <th>YearBuilt</th>\n      <th>YearRemodAdd</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2515</td>\n      <td>0</td>\n      <td>4</td>\n      <td>No</td>\n      <td>1219</td>\n      <td>Rec</td>\n      <td>816</td>\n      <td>484</td>\n      <td>Unf</td>\n      <td>1975</td>\n      <td>...</td>\n      <td>TA</td>\n      <td>32668</td>\n      <td>69</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>6</td>\n      <td>2035</td>\n      <td>1957</td>\n      <td>1975</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>958</td>\n      <td>620</td>\n      <td>3</td>\n      <td>No</td>\n      <td>403</td>\n      <td>BLQ</td>\n      <td>238</td>\n      <td>240</td>\n      <td>Unf</td>\n      <td>1941</td>\n      <td>...</td>\n      <td>Fa</td>\n      <td>9490</td>\n      <td>79</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7</td>\n      <td>6</td>\n      <td>806</td>\n      <td>1941</td>\n      <td>1950</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>979</td>\n      <td>224</td>\n      <td>3</td>\n      <td>No</td>\n      <td>185</td>\n      <td>LwQ</td>\n      <td>524</td>\n      <td>352</td>\n      <td>Unf</td>\n      <td>1950</td>\n      <td>...</td>\n      <td>Gd</td>\n      <td>7015</td>\n      <td>69</td>\n      <td>161</td>\n      <td>0</td>\n      <td>4</td>\n      <td>5</td>\n      <td>709</td>\n      <td>1950</td>\n      <td>1950</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1156</td>\n      <td>866</td>\n      <td>4</td>\n      <td>No</td>\n      <td>392</td>\n      <td>BLQ</td>\n      <td>768</td>\n      <td>505</td>\n      <td>Fin</td>\n      <td>1977</td>\n      <td>...</td>\n      <td>TA</td>\n      <td>10005</td>\n      <td>83</td>\n      <td>299</td>\n      <td>117</td>\n      <td>5</td>\n      <td>7</td>\n      <td>1160</td>\n      <td>1977</td>\n      <td>1977</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>525</td>\n      <td>0</td>\n      <td>3</td>\n      <td>No</td>\n      <td>0</td>\n      <td>Unf</td>\n      <td>525</td>\n      <td>264</td>\n      <td>Unf</td>\n      <td>1971</td>\n      <td>...</td>\n      <td>TA</td>\n      <td>1680</td>\n      <td>21</td>\n      <td>381</td>\n      <td>0</td>\n      <td>5</td>\n      <td>6</td>\n      <td>525</td>\n      <td>1971</td>\n      <td>1971</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 21 columns</p>\n</div>"
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "TrainSet_df = pd.read_csv(\"outputs/datasets/cleaned/train/TrainSetCleaned.csv\")\n",
        "TestSet_df = pd.read_csv(\"outputs/datasets/cleaned/test/TestSetCleaned.csv\")\n",
        "\n",
        "# Check the first few rows of the DataFrame to confirm that it loaded correctly\n",
        "TrainSet_df.head()\n",
        "TestSet_df.head()"
      ]
    },
    {
      "source": [],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "First we look at encoding our categorical variables since many machine learning models require inputs to be numerical. Luckily, most of our variables are numerical already, but we do have 4 categorical variables that need to be converted into numerical form.\n",
        "\n",
        "We will use Ordinal Encoding. This method converts each category to a unique integer. It is suitable for ordinal variables, where there is an inherent order in the categories.\n",
        "\n",
        "Looking at our categorical variables:\n",
        "\n",
        "**BsmtExposure**, **BsmtFinType1**, **GarageFinish** seem to be ordinal variables as they indicate quality or condition, with 'None' likely representing a missing or not applicable condition.\n",
        "\n",
        "**KitchenQual** seems to be an ordinal variable as well, with an inherent order (Ex > Gd > TA > Fa). \n",
        "\n",
        "Great!"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "# Categorical encoding"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unhashable type: 'dict_keys'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m TestSet_df_encoded \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(encoder\u001b[38;5;241m.\u001b[39mtransform(TestSet_df[categories_dict\u001b[38;5;241m.\u001b[39mkeys()]), columns\u001b[38;5;241m=\u001b[39mcategories_dict\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Replace the old categorical columns in the dataframes with the new encoded ones\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[43mTrainSet_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcategories_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m TrainSet_df_encoded\n\u001b[1;32m     28\u001b[0m TestSet_df[categories_dict\u001b[38;5;241m.\u001b[39mkeys()] \u001b[38;5;241m=\u001b[39m TestSet_df_encoded\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py:3645\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3643\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array(key, value)\n\u001b[1;32m   3644\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, DataFrame):\n\u001b[0;32m-> 3645\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item_frame_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3646\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m   3647\u001b[0m     is_list_like(value)\n\u001b[1;32m   3648\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique\n\u001b[1;32m   3649\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_indexer_for([key])) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(value)\n\u001b[1;32m   3650\u001b[0m ):\n\u001b[1;32m   3651\u001b[0m     \u001b[38;5;66;03m# Column to set is duplicated\u001b[39;00m\n\u001b[1;32m   3652\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py:3770\u001b[0m, in \u001b[0;36mDataFrame._set_item_frame_value\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3767\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_valid_index(value)\n\u001b[1;32m   3769\u001b[0m \u001b[38;5;66;03m# align columns\u001b[39;00m\n\u001b[0;32m-> 3770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m:\n\u001b[1;32m   3771\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   3772\u001b[0m     cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns[loc]\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py:5008\u001b[0m, in \u001b[0;36mIndex.__contains__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4973\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__contains__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m   4974\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4975\u001b[0m \u001b[38;5;124;03m    Return a boolean indicating whether the provided key is in the index.\u001b[39;00m\n\u001b[1;32m   4976\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5006\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[1;32m   5007\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5008\u001b[0m     \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5009\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   5010\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict_keys'"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# Define a dictionary with the correct order of each category for each variable\n",
        "categories_dict = {\n",
        "    'BsmtExposure': ['None', 'No', 'Mn', 'Av', 'Gd'],\n",
        "    'BsmtFinType1': ['None', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n",
        "    'GarageFinish': ['None', 'Unf', 'RFn', 'Fin'],\n",
        "    'KitchenQual': ['Fa', 'TA', 'Gd', 'Ex'],\n",
        "}\n",
        "\n",
        "# Convert dict_keys object to a list\n",
        "categories_list = list(categories_dict.keys())\n",
        "\n",
        "# Instantiate the encoder\n",
        "encoder = OrdinalEncoder(categories=list(categories_dict.values()))\n",
        "\n",
        "# Fit the encoder on the categorical columns of the training data\n",
        "encoder.fit(TrainSet_df[categories_dict.keys()])\n",
        "\n",
        "# Apply the encoder to the training data\n",
        "TrainSet_df_encoded = pd.DataFrame(encoder.transform(TrainSet_df[categories_dict.keys()]), columns=categories_dict.keys())\n",
        "\n",
        "# Apply the encoder to the test data\n",
        "TestSet_df_encoded = pd.DataFrame(encoder.transform(TestSet_df[categories_dict.keys()]), columns=categories_dict.keys())\n",
        "\n",
        "# Replace the old categorical columns in the dataframes with the new encoded ones\n",
        "TrainSet_df[categories_dict.keys()] = TrainSet_df_encoded\n",
        "TestSet_df[categories_dict.keys()] = TestSet_df_encoded\n"
      ]
    },
    {
      "source": [
        "# Feature Engineering Analysis"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from feature_engine import transformation as vt\n",
        "from feature_engine.outliers import Winsorizer\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "sns.set(style=\"whitegrid\")\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "def FeatureEngineeringAnalysis(df, analysis_type='numerical'):\n",
        "    \"\"\"\n",
        "    - used for quick feature engineering on numerical and categorical variables\n",
        "    to decide which transformation can better transform the distribution shape\n",
        "    - Once transformed, use a reporting tool, like pandas-profiling, to evaluate distributions\n",
        "    \"\"\"\n",
        "    check_missing_values(df)\n",
        "    allowed_types = ['numerical', 'ordinal_encoder', 'outlier_winsorizer']\n",
        "    check_user_entry_on_analysis_type(analysis_type, allowed_types)\n",
        "    list_column_transformers = define_list_column_transformers(analysis_type)\n",
        "\n",
        "    # Loop in each variable and engineer the data according to the analysis type\n",
        "    df_feat_eng = pd.DataFrame([])\n",
        "    for column in df.columns:\n",
        "        # create additional columns (column_method) to apply the methods\n",
        "        df_feat_eng = pd.concat([df_feat_eng, df[column]], axis=1)\n",
        "        for method in list_column_transformers:\n",
        "            df_feat_eng[f\"{column}_{method}\"] = df[column]\n",
        "\n",
        "        # Apply transformers in respective column_transformers\n",
        "        df_feat_eng, list_applied_transformers = apply_transformers(\n",
        "            analysis_type, df_feat_eng, column)\n",
        "\n",
        "        # For each variable, assess how the transformations perform\n",
        "        transformer_evaluation(\n",
        "            column, list_applied_transformers, analysis_type, df_feat_eng)\n",
        "\n",
        "    return df_feat_eng\n",
        "\n",
        "\n",
        "def check_user_entry_on_analysis_type(analysis_type, allowed_types):\n",
        "    \"\"\" Check analysis type \"\"\"\n",
        "    if analysis_type is None:\n",
        "        raise SystemExit(\n",
        "            f\"You should pass analysis_type parameter as one of the following options: {allowed_types}\")\n",
        "    if analysis_type not in allowed_types:\n",
        "        raise SystemExit(\n",
        "            f\"analysis_type argument should be one of these options: {allowed_types}\")\n",
        "\n",
        "\n",
        "def check_missing_values(df):\n",
        "    if df.isna().sum().sum() != 0:\n",
        "        raise SystemExit(\n",
        "            f\"There is a missing value in your dataset. Please handle that before getting into feature engineering.\")\n",
        "\n",
        "\n",
        "def define_list_column_transformers(analysis_type):\n",
        "    \"\"\" Set suffix columns according to analysis_type\"\"\"\n",
        "    if analysis_type == 'numerical':\n",
        "        list_column_transformers = [\n",
        "            \"log_e\", \"log_10\", \"reciprocal\", \"power\", \"box_cox\", \"yeo_johnson\"]\n",
        "\n",
        "    elif analysis_type == 'ordinal_encoder':\n",
        "        list_column_transformers = [\"ordinal_encoder\"]\n",
        "\n",
        "    elif analysis_type == 'outlier_winsorizer':\n",
        "        list_column_transformers = ['iqr']\n",
        "\n",
        "    return list_column_transformers\n",
        "\n",
        "\n",
        "def apply_transformers(analysis_type, df_feat_eng, column):\n",
        "    for col in df_feat_eng.select_dtypes(include='category').columns:\n",
        "        df_feat_eng[col] = df_feat_eng[col].astype('object')\n",
        "\n",
        "    if analysis_type == 'numerical':\n",
        "        df_feat_eng, list_applied_transformers = FeatEngineering_Numerical(\n",
        "            df_feat_eng, column)\n",
        "\n",
        "    elif analysis_type == 'outlier_winsorizer':\n",
        "        df_feat_eng, list_applied_transformers = FeatEngineering_OutlierWinsorizer(\n",
        "            df_feat_eng, column)\n",
        "\n",
        "    elif analysis_type == 'ordinal_encoder':\n",
        "        df_feat_eng, list_applied_transformers = FeatEngineering_CategoricalEncoder(\n",
        "            df_feat_eng, column)\n",
        "\n",
        "    return df_feat_eng, list_applied_transformers\n",
        "\n",
        "\n",
        "def transformer_evaluation(column, list_applied_transformers, analysis_type, df_feat_eng):\n",
        "    # For each variable, assess how the transformations perform\n",
        "    print(f\"* Variable Analyzed: {column}\")\n",
        "    print(f\"* Applied transformation: {list_applied_transformers} \\n\")\n",
        "    for col in [column] + list_applied_transformers:\n",
        "\n",
        "        if analysis_type != 'ordinal_encoder':\n",
        "            DiagnosticPlots_Numerical(df_feat_eng, col)\n",
        "\n",
        "        else:\n",
        "            if col == column:\n",
        "                DiagnosticPlots_Categories(df_feat_eng, col)\n",
        "            else:\n",
        "                DiagnosticPlots_Numerical(df_feat_eng, col)\n",
        "\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n",
        "def DiagnosticPlots_Categories(df_feat_eng, col):\n",
        "    plt.figure(figsize=(4, 3))\n",
        "    sns.countplot(data=df_feat_eng, x=col, palette=[\n",
        "                  '#432371'], order=df_feat_eng[col].value_counts().index)\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.suptitle(f\"{col}\", fontsize=30, y=1.05)\n",
        "    plt.show()\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "def DiagnosticPlots_Numerical(df, variable):\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "    sns.histplot(data=df, x=variable, kde=True, element=\"step\", ax=axes[0])\n",
        "    stats.probplot(df[variable], dist=\"norm\", plot=axes[1])\n",
        "    sns.boxplot(x=df[variable], ax=axes[2])\n",
        "\n",
        "    axes[0].set_title('Histogram')\n",
        "    axes[1].set_title('QQ Plot')\n",
        "    axes[2].set_title('Boxplot')\n",
        "    fig.suptitle(f\"{variable}\", fontsize=30, y=1.05)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def FeatEngineering_CategoricalEncoder(df_feat_eng, column):\n",
        "    list_methods_worked = []\n",
        "    try:\n",
        "        encoder = OrdinalEncoder(encoding_method='arbitrary', variables=[\n",
        "                                 f\"{column}_ordinal_encoder\"])\n",
        "        df_feat_eng = encoder.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_ordinal_encoder\")\n",
        "\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_ordinal_encoder\"], axis=1, inplace=True)\n",
        "\n",
        "    return df_feat_eng, list_methods_worked\n",
        "\n",
        "\n",
        "def FeatEngineering_OutlierWinsorizer(df_feat_eng, column):\n",
        "    list_methods_worked = []\n",
        "\n",
        "    # Winsorizer iqr\n",
        "    try:\n",
        "        disc = Winsorizer(\n",
        "            capping_method='iqr', tail='both', fold=1.5, variables=[f\"{column}_iqr\"])\n",
        "        df_feat_eng = disc.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_iqr\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_iqr\"], axis=1, inplace=True)\n",
        "\n",
        "    return df_feat_eng, list_methods_worked\n",
        "\n",
        "\n",
        "def FeatEngineering_Numerical(df_feat_eng, column):\n",
        "    list_methods_worked = []\n",
        "\n",
        "    # LogTransformer base e\n",
        "    try:\n",
        "        lt = vt.LogTransformer(variables=[f\"{column}_log_e\"])\n",
        "        df_feat_eng = lt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_log_e\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_log_e\"], axis=1, inplace=True)\n",
        "\n",
        "    # LogTransformer base 10\n",
        "    try:\n",
        "        lt = vt.LogTransformer(variables=[f\"{column}_log_10\"], base='10')\n",
        "        df_feat_eng = lt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_log_10\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_log_10\"], axis=1, inplace=True)\n",
        "\n",
        "    # ReciprocalTransformer\n",
        "    try:\n",
        "        rt = vt.ReciprocalTransformer(variables=[f\"{column}_reciprocal\"])\n",
        "        df_feat_eng = rt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_reciprocal\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_reciprocal\"], axis=1, inplace=True)\n",
        "\n",
        "    # PowerTransformer\n",
        "    try:\n",
        "        pt = vt.PowerTransformer(variables=[f\"{column}_power\"])\n",
        "        df_feat_eng = pt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_power\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_power\"], axis=1, inplace=True)\n",
        "\n",
        "    # BoxCoxTransformer\n",
        "    try:\n",
        "        bct = vt.BoxCoxTransformer(variables=[f\"{column}_box_cox\"])\n",
        "        df_feat_eng = bct.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_box_cox\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_box_cox\"], axis=1, inplace=True)\n",
        "\n",
        "    # YeoJohnsonTransformer\n",
        "    try:\n",
        "        yjt = vt.YeoJohnsonTransformer(variables=[f\"{column}_yeo_johnson\"])\n",
        "        df_feat_eng = yjt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_yeo_johnson\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_yeo_johnson\"], axis=1, inplace=True)\n",
        "\n",
        "    return df_feat_eng, list_methods_worked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_transformed \u001b[38;5;241m=\u001b[39m FeatureEngineeringAnalysis(\u001b[43mdf\u001b[49m, analysis_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumerical\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# replace 'numerical' with the analysis type you want\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "df_transformed = FeatureEngineeringAnalysis(df, analysis_type='numerical')  # replace 'numerical' with the analysis type you want\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Next step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Great! Now you can  push the changes to your GitHub Repo, using the Git commands (git add, git commit, git push)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python381264bit3812pyenvc44d0ba3d4d74926a1a78ae71e39f6ac"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12-final"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}